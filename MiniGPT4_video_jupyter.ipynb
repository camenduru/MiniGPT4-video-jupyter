{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/MiniGPT4-video-jupyter/blob/main/MiniGPT4_video_jupyter.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone -b dev https://github.com/camenduru/MiniGPT4-video\n",
        "%cd /content/MiniGPT4-video\n",
        "\n",
        "!apt -y install -qq aria2\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Vision-CAIR/MiniGPT4-Video/resolve/main/checkpoints/video_llama_checkpoint_last.pth -d /content/MiniGPT4-video/pretrained_models -o video_llama_checkpoint_last.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Vision-CAIR/MiniGPT4-Video/resolve/main/checkpoints/video_mistral_checkpoint_last.pth -d /content/MiniGPT4-video/pretrained_models -o video_mistral_checkpoint_last.pth\n",
        "\n",
        "!pip install -q webvtt-py==0.4.6 omegaconf==2.3.0 iopath==0.1.10 timm==0.6.13  webdataset==0.2.48 visual-genome==1.1.1 decord==0.6.0 pysrt==1.1.2 peft==0.2.0 wandb==0.14.2 openai==0.28.0 pycocoevalcap==1.2\n",
        "!pip install -q pytubefix transformers==4.37.2 moviepy==1.0.3 nltk==3.8.1 soundfile==0.12.1 sentencepiece==0.1.97 bitsandbytes==0.42.0 accelerate==0.25.0 opencv-python==4.7.0.72 scikit-image==0.22.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import webvtt\n",
        "import cv2 \n",
        "from minigpt4.common.eval_utils import init_model\n",
        "from minigpt4.conversation.conversation import CONV_VISION\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "import soundfile as sf\n",
        "import moviepy.editor as mp\n",
        "from PIL import Image\n",
        "from moviepy.editor import VideoFileClip\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "class Options:\n",
        "    def __init__(self):\n",
        "        self.cfg_path = \"/content/MiniGPT4-video/test_configs/llama2_test_config.yaml\"\n",
        "        self.ckpt = '/content/MiniGPT4-video/pretrained_models/video_llama_checkpoint_last.pth'\n",
        "        self.add_subtitles = False\n",
        "        self.question = None\n",
        "        self.video_path = None\n",
        "        self.max_new_tokens = 512\n",
        "        self.lora_r = 64\n",
        "        self.lora_alpha = 16\n",
        "        self.options = None\n",
        "\n",
        "args=Options()\n",
        "\n",
        "def prepare_input(vis_processor,video_path,subtitle_path,instruction):  \n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if subtitle_path is not None: \n",
        "        # Load the VTT subtitle file\n",
        "        vtt_file = webvtt.read(subtitle_path) \n",
        "        print(\"subtitle loaded successfully\")  \n",
        "        clip = VideoFileClip(video_path)\n",
        "        total_num_frames = int(clip.duration * clip.fps)\n",
        "        # print(\"Video duration = \",clip.duration)\n",
        "        clip.close()\n",
        "    else :\n",
        "        # calculate the total number of frames in the video using opencv        \n",
        "        total_num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) \n",
        "    max_images_length = 45\n",
        "    max_sub_len = 400\n",
        "    images = []\n",
        "    frame_count = 0\n",
        "    sampling_interval = int(total_num_frames / max_images_length)\n",
        "    if sampling_interval == 0:\n",
        "        sampling_interval = 1\n",
        "    img_placeholder = \"\"\n",
        "    subtitle_text_in_interval = \"\"\n",
        "    history_subtitles = {}\n",
        "    raw_frames=[]\n",
        "    number_of_words=0\n",
        "    transform=transforms.Compose([transforms.ToPILImage(),])\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        # Find the corresponding subtitle for the frame and combine the interval subtitles into one subtitle\n",
        "        # we choose 1 frame for every 2 seconds,so we need to combine the subtitles in the interval of 2 seconds\n",
        "        if subtitle_path is not None: \n",
        "            for subtitle in vtt_file:\n",
        "                sub=subtitle.text.replace('\\n',' ')\n",
        "                if (subtitle.start_in_seconds <= (frame_count / int(clip.fps)) <= subtitle.end_in_seconds) and sub not in subtitle_text_in_interval:\n",
        "                    if not history_subtitles.get(sub,False):\n",
        "                        subtitle_text_in_interval+=sub+\" \"\n",
        "                    history_subtitles[sub]=True\n",
        "                    break\n",
        "        if frame_count % sampling_interval == 0:\n",
        "            raw_frames.append(Image.fromarray(cv2.cvtColor(frame.copy(), cv2.COLOR_BGR2RGB)))\n",
        "            frame = transform(frame[:,:,::-1]) # convert to RGB\n",
        "            frame = vis_processor(frame)\n",
        "            images.append(frame)\n",
        "            img_placeholder += '<Img><ImageHere>'\n",
        "            if subtitle_path is not None and subtitle_text_in_interval != \"\" and number_of_words< max_sub_len: \n",
        "                img_placeholder+=f'<Cap>{subtitle_text_in_interval}'\n",
        "                number_of_words+=len(subtitle_text_in_interval.split(' '))\n",
        "                subtitle_text_in_interval = \"\"\n",
        "        frame_count += 1\n",
        "\n",
        "        if len(images) >= max_images_length:\n",
        "            break\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    if len(images) == 0:\n",
        "        # skip the video if no frame is extracted\n",
        "        return None,None\n",
        "    images = torch.stack(images)\n",
        "    instruction = img_placeholder + '\\n' + instruction\n",
        "    return images,instruction\n",
        "def extract_audio(video_path, audio_path):\n",
        "    video_clip = mp.VideoFileClip(video_path)\n",
        "    audio_clip = video_clip.audio\n",
        "    audio_clip.write_audiofile(audio_path, codec=\"libmp3lame\", bitrate=\"320k\")\n",
        "    \n",
        "def generate_subtitles(video_path):\n",
        "    video_id=video_path.split('/')[-1].split('.')[0]\n",
        "    audio_path = f\"workspace/inference_subtitles/mp3/{video_id}\"+'.mp3'\n",
        "    os.makedirs(\"workspace/inference_subtitles/mp3\",exist_ok=True)\n",
        "    if existed_subtitles.get(video_id,False):\n",
        "        return f\"workspace/inference_subtitles/{video_id}\"+'.vtt'\n",
        "    try:\n",
        "        extract_audio(video_path,audio_path)\n",
        "        print(\"successfully extracted\")\n",
        "        os.system(f\"whisper {audio_path}  --language English --model large --output_format vtt --output_dir workspace/inference_subtitles\")\n",
        "        # remove the audio file\n",
        "        os.system(f\"rm {audio_path}\")\n",
        "        print(\"subtitle successfully generated\")  \n",
        "        return f\"workspace/inference_subtitles/{video_id}\"+'.vtt'\n",
        "    except Exception as e:\n",
        "        print(\"error\",e)\n",
        "        print(\"error\",video_path)\n",
        "        return None\n",
        "\n",
        "def run (video_path,instruction,model,vis_processor,gen_subtitles=True):\n",
        "    if gen_subtitles:\n",
        "        subtitle_path=generate_subtitles(video_path)\n",
        "    else :\n",
        "        subtitle_path=None\n",
        "    prepared_images,prepared_instruction=prepare_input(vis_processor,video_path,subtitle_path,instruction)\n",
        "    if prepared_images is None:\n",
        "        return \"Video cann't be open ,check the video path again\"\n",
        "    length=len(prepared_images)\n",
        "    prepared_images=prepared_images.unsqueeze(0)\n",
        "    conv = CONV_VISION.copy()\n",
        "    conv.system = \"\"\n",
        "    # if you want to make conversation comment the 2 lines above and make the conv is global variable\n",
        "    conv.append_message(conv.roles[0], prepared_instruction)\n",
        "    conv.append_message(conv.roles[1], None)\n",
        "    prompt = [conv.get_prompt()]\n",
        "    answers = model.generate(prepared_images, prompt, max_new_tokens=args.max_new_tokens, do_sample=True, lengths=[length],num_beams=1)\n",
        "    return answers[0]\n",
        "\n",
        "def setup_seeds(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    cudnn.benchmark = False\n",
        "    cudnn.deterministic = True\n",
        "\n",
        "import yaml \n",
        "with open('test_configs/llama2_test_config.yaml') as file:\n",
        "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
        "seed=config['run']['seed']\n",
        "print(\"seed\",seed)\n",
        "\n",
        "model, vis_processor = init_model(args)\n",
        "conv = CONV_VISION.copy()\n",
        "conv.system = \"\"\n",
        "inference_subtitles_folder=\"inference_subtitles\"\n",
        "os.makedirs(inference_subtitles_folder,exist_ok=True)\n",
        "existed_subtitles={}\n",
        "for sub in os.listdir(inference_subtitles_folder):\n",
        "    existed_subtitles[sub.split('.')[0]]=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "args.video_path = '/content/ngp_ep0019_audio.mp4'\n",
        "args.question = \"What's this video talking about?\"\n",
        "args.add_subtitles = False\n",
        "\n",
        "pred=run(args.video_path, args.question, model, vis_processor, gen_subtitles=args.add_subtitles)\n",
        "print(pred)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
